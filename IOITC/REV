Cake:

Number of parts the circle is cut = i + l + 1

l = number of segments 
i = number of intersection points inside a circle
 S = set of all unique intersection points made by the segments.
Some of the points can be outside the circle, so we remove all the point with distance more 
than r (approximate) from the center.


SumThing: 

We have two choices, putting a plus sign before an element 'i' or not putting it. If we put a plus sign before i then DP[i][k] = 1 + DP[i-1][k-digit[i]]. Otherwise we find out a position 'j' such that DP[j-1][k-num]+1 is minimized where num is the number formed by digits from j to i and put a plus sign before j. This will take us O(n) time but notice that if we form num in increasing order (if we search for j from i-1 to 0) then after num exceeds k, we don't need to search any further because all other numbers formed will be greater than num. So that takes only O(log10( k)) time. K can be at most 10^5 so log10(k) will be only 5 in the worst case. So the total time is O(N*Klog10(k)). Also the algorithm will put a plus sign before the first digit so we need to subtract one from the final answer.

BarCode:

Converting the problem from two dimensions to one dimension: As each column can be only '#' or '.', we pre-calculate the cost of changing every column to 0 and changing it to 1. Using this we find the minimum cost of satisfying the required conditions using dynamic programming. For each column 'i', we have two choices, either the column is '#' or it is '.'. To calculate the minimum cost of having column i as '#', we find an index j such that all column from j to i are '#' and the cost of transforming them to '#' is minimized. j has to be at least X-1 columns away from i and at most Y-1 columns away form i and column j-1 has to be '.'. Such a value 'j' is always valid iff i is greater than X+Y. So for the first X+Y indices we calculate the minimum cost of changing them to '#' separately. The cost of changing a column to '.'  is calculated in the same way.

Sorted Vectors:

We can use topological sort to sort the vectors in the order described. There is an edge from vector 'a' to vector 'b' if for a coordinate 'i' a[i] < b[i] but there does not exist a vector 'c' such that a[i] < c[i] < b[i]. If there are many vectors which have a[i] or b[i] as their i th coordinate then there can be a quadratic number of edges that if why we don't put a direct edge from a to b but we create a dummy vertex v and connect a and b through v (a-v-b). Each vertex will have an 'id' which stores its position in a lexicographic ordering. For all the real vertices id[i] = i, and for the dummy vertices, id[i] will be the minimum id of a vector to which it has an outgoing arc. We then sort them in topological order using a priority queue where the vertex with lowest id has the highest priority.

Flowery Graphs:

We need to consider each node as a bud and take the K maximum heights of its subtrees to form stems. This can be done in O(N^2) if we use dfs to find the heights its subtrees. But if we create the tree, rooted at an arbitrary vertex, all the subtrees of that vertex will be rooted at its children. For all other nodes, all of their subtrees except one will be rooted at their children, that one subtree will be rooted at their parent. So we only need to find the height of the subtree rooted at every node's parent. For this we store the two maximum heights of the subtrees m1,m2 rooted at every node. The height of the subtree rooted at a node 'n''s parent 'p' is m1[p] if the height of n is less than (not equal) m1[p], m2[p] otherwise. We update the m1,m2 values of n appropriately and consider the height of its parent in our K maximum heights.  Total time complexity: O(NlogN).

Police:

Label the nodes of the tree such that every parent node has a number lower than its child node. At every node store then range of the labels in its subtree. Compute all the critical edges and critical nodes. At every critical node, store the ranges of all its subtrees which are disconnected when the node is removed, in sorted order. If the node/edge removed in the queries is a not critical then we can always go from a to b. Otherwise we can go from a to b iff they lie in the same component. If an edge is removed and v is the endpoint of the edge with the higher label then we check if a and b both lie in the ranges of v, if only one of them lies in the subtree rooted at v, then we cannot go from a to b, otherwise we can. If a node n is removed, we binary search for the subtrees a and b lie in (using the ranges of n's disconnected subtrees). If they lie in the same subtree or both of them do not lie in any of the subtrees, then we can go from a to b, we cannot otherwise.

Circulation:

If we separate all the strongly connected components in the graph, we get a DAG where nodes are the SCCs. Every SCC is just a cycle consisting of all the vertices exactly once, because as the problem states there is only one cycle from any vertex back to itself, thus we can prove that every SCC is formed by a cycle of all the vertices in it. For every vertex, we compute the longest path starting from it.

L[v] = the longest path starting from v and going out of its SCC
F[v] = the longest path from v

If there is an edge from v to u where u is in another component (the edge v-u is an edge in the DAG) then L[v] = 1+F[u]. Otherwise L[v] = 0.
For every vertex v F[v] = max(L[v+1]+1,L[v+2]+2...L[v+n-1]+n-1) where n is the number of vertices in the SCC of v. If we use the above formula for every vertex the algorithm will take O(N^2) time, but as the SCC is a cycle we can compute this quantity in linear time.

Let a->b->c->d->e->a be the SCC then:

F[a] = max(			L[a]+0,L[b]+1,L[c]+2,L[d]+3,L[e]+4)
F[b] = max(L[a]+4,		L[b]+0,L[c]+1,L[d]+2,L[e]+3)
F[c] = max(L[a]+3,L[b]+4,	L[c]+0,L[d]+1,L[e]+2).....

we can write it as:

M[a] = max(L[a]+0,L[b]+1,L[c]+2,L[d]+3,L[e]+4);
T[a] = 0;
F[a] = max(T[a],M[a])
M[b] = M[a]-1
T[b] = max(T[a]-1,L[a]+4)
F[b] = max(T[b],M[b])
M[c] = M[b]-1
T[c] = max(T[b]-1,L[b]+4)
F[c] = max(T[c],M[c])

thus we have the recurrence,

T[v] = max(T[v-1]-1,L[v-1]+n-1)
M[v] = M[v-1]-1
F[v] = max(M[v],T[v])

So the total runtime of the algorithm is linear.

Register

We sort the array A and store the pointers to (and from) the indices of the original array.
We maintain a Segment Tree where the element at A[i] is the sum of the numbers stored at each node on the path from the root to the leaf of i. To perform operation 1, we find out the value in the sorted array which is pointed by 'i', A[k] (O(logn) time). Then we binary search* on the segment tree to find the last occurrence of A[k], j. We swap the pointers pointing at k and j (so that now i points to j.) and increment A[j] by one which does not change the ordering of A. To perform operation 2, we simply perform a binary search to find the first index greater or equal to than x, k and return N-k, where N is the number of elements in A. Operation 3, notice that this operation also, does not change the ordering of A. To perform this operation, we first binary search for the first occurrence of y, k and then query the segment tree for the range k..N-1 and decrement the number at every node which falls in the range.

*This binary search will be just like a normal binary search on A, but to get the value of A[i], we will have to go from the leaf of i to the root of the tree adding up the numbers on all the nodes we encounter, which takes O(logn) time. So the total search take O(loglogn) time. And so, the time complexity for each operation will be O(loglogn).


Walk:

Notice that every 'walk' from s to t is around the corners of the rectangles (if they exist). Thus we can use the corner points of the rectangles as vertices and find the shortest path from s to t. To do this we first have to create edges in such a way that the number of edges is linear to the number of vertices (i.e. the corners of the rectangles) and it is possible to go from any vertex to any other. 

When can we not go from a point to another via the shortest path?

------------    -----------------
|       .b |	|	.c	|		
|	   |	|	|    .b |
|.a	   |	|	|	| fig.(2)
------------	| .a	|	|
    fig.(1)	|	.d	|
		-----------------

In the above figures a and b are two points and c-d is a line segment. In fig.(1) we can from a to b via the shortest path (left and then up), but in fig.(2), our path is blocked by the line segment c-d so in order to go from a to b, we first have to go up from a to c(or d) and then go down from c(d) to b. So we cannot go from a to b (b is higher than a) via the shortest path iff there is a line segment(rectangle) c-d such that d is lower than a and c is higher than b.

So we have a sweeping line which stores the ranges currently blocked, when we insert a line segment, we connect it to all the segments to the left of it which are not blocked. As we have to keep the number of edges linear in terms of the number of vertices, we only connect the line segments from the first line segment that is above the current segment to the first line segment that is below it. (Repeat the same s thing with line segments parallel to the X-axis) Then we can run a O(ElogV) shortest path algorithm from point s and t to get the shortest path.

Grid:

If we have a line from the origin which starting from the X-axis go in the anti clockwise direction till the Y-axis, we can find out the maximum rectangles intersected by the line at any point. Notice that this sweep line starts intersecting a rectangles at its lower right corner and stops intersecting it at its upper left corner. So we store the polar angles of the lower right and the upper left corners of every rectangle and sort them from lowest to highest. We have a sweep line as described above where the interesting points are the polar angles which we have stored. Whenever we see a lower right corner, we increment our count of intersecting rectangles and whenever we see an upper left corner, we decrement it and keep track of the position where the count is highest. We will have the polar angles of the line where intersection is maximum, we find its intersection with the right or the upper boundary of the grid and return it. To do this we can just keep track of the coordinates of the lower right corner which was added when we got this maximum count and find its intersection with the two boundaries and take the point which lies on the boundary.

Vacations:

How do we know whether a number is unique or not?
1. For each value, keeping track of which positions it occurs.
2. By keeping track of whether the number exists or not.
3. By storing the last occurrence of a number.

A an element is unique iff it is the first occurrence of that value, so we keep track of where we have seen that value before. So for each element we store the index of the previous occurrence of the number or -1 if we haven't seen it yet. Now we can convert a query of finding the number of unique elements in a range a b to finding the number of elements in range a b less than a, which we can easily do by using a segment tree and merge sort. At every node in the segment tree, we store the sorted array of all the elements in that range, and when we query for a particular range, we binary search on all the nodes we find in the given range. Total time complexity O(loglogn) for each query.

Brackets:

We use three stacks (C,H,W) to determine the height, width and color of every pair of brackets. When we se an opening bracket, we insert a 'undefined' in all three stacks. When we see a closing bracket we pop of the elements of the stack until we see an undefined and store the values which we have seen so far. Set the undefined value in stack C to the smallest value c, which is not equal to the values we have stored. In the stack H we set the undefined value to h, which is the maximum value we have seen so far plus 2.
In the stack W we set the undefined value to w, which is the sum of all the values plus 1. As every element enters and leaves the stack only once, this takes O(N) time. Now we increment the count of the color c by (h*2) + (w*4 -3) which is just the number of squares of that color corresponding to that particular pair.

If we observe how many pairs of brackets it takes to have k+1 colors which is given by ‚ƒ(k), where k is the maximum color we have seen so far: ‚ƒ(0) = 1 , ‚ƒ(1) = 2 ,‚ƒ(2) = 4, ‚ƒ(3) = 8, ‚ƒ(4) = 16........., we can conclude that ‚ƒ(k) = 2^k, so the number of color we can have with N pairs is just log2(N). So the number of colors which we can have will be at most log2(3*10^6) which is just 22, so we can easily try all subsets of K where K is the set of all the colors we have. Which we can do even more easily by using a 32-bit integer whose nth bit tells us whether or not to include the nth color. And so the total time complexity still remains O(N).


Segment:

Algorithm 1: We convert the sequence to a sequence where consecutive elements have opposite signs by grouping together consecutive numbers of the same size as a single number representing their sum. Now we will have 'k' elements(groups) with a negative sign and if we flip all of them, we will get the sequence with maximum sum. But we are allowed to flip only K segments, to reduce the number of elements we have to either flip some positive numbers along with their negative neighbors or not flip a negative number, the greedy algorithm proceeds as follows:
First we discard the positive numbers (if any) at the two endpoints of the sequence and store them. At each step we choose the element(group) with minimum absolute value. If it is a positive number, we choose to flip it, thus combining it with its two neighbors by adding them up and reducing k by 1. If it is a negative number then we just combine it with its positive neighbors in the same way which again reduces k by 1. If in this process any positive numbers at the endpoints are created, we just discard them(and add them later to the final sum). It can happen that an element is chosen not to flip and then again chosen to flip. We repeat the above until k is equal to K and then just add up the absolute values of the remaining elements and the discarded elements. To do this efficiently we need to have a linked list to have fast deletions and maintain the order of the elements. To find the element with minimum cost we have to use a heap. So the total running time of the algorithm will be O(NlogN).

Farm: 

We can always extend an edge of one of the two convex hulls such that both the convex hulls lie on either sides of that line. We can test all pairs of points to be that line and then partition the remaining points into the two convex hulls. The points lying on the line can be put into either of the two convex hulls, we just try both the possibilities and keep track of the best area we get. There will be N^2 such pairs and finding the convex hull and its area will take O(N^2) time, so the total time complexity is O(N^4) which will run well within the time limits as N ≤ 50.

Carnival:

If there exists a solution of k rides can always find another (or same) solution of k rides by taking the rides in increasing order of their required height. So we sort the rides in ascending order of their height.

Let f[i][j] be the maximum height we can achieve by taking i rides with a budget of j then:
L[i+1][j+C[i]] = max(L[i+1][j+C[i]],L[i][j]+H[i]) if L[i][j] >= T[i]. The matrix L is initialized to 0 except for L[0][0] which is initialized to H.

We find the highest index i such that L[i][j] is nonzero, for any j ≤ M. This is the maximum number of rides we can take. The total complexity of this algorithm is O(N^2*M).

Move:

We assign a value 'key' to every number in U initially key[i] = i.  We also store the smallest key value assigned so far, which is initially 0. S is stored as a binary heap where elements are ordered according to their key value and the value with minimum key is on top. Insert() and Delete() take O(logn) time. We keep track of whether a number is in the heap or not. To make x the new minimum in U, we set the key value of x to one less than our smallest key value and update the smallest key value. If x is in S then we move x up to its appropriate position. Min() takes O(1) time, just return the element on top of the heap. Insert(), Delete() and MoveToF take O(logn) time. Alternatively we can do the same using std::set, the time complexities remain the same.

Optree:

P[] - the list of parent nodes.
D - the distance from the source node.

We run the shortest path algorithm from the source and every time there is an edge from v to u and D[u] > D[v] + weight(v,u) we clear the list P[u] and insert v into it. If D[u] = D[v] + weight(v,u) then we just insert v into the list P[u].
Now we will have a DAG created by the edges from the parent nodes of every node.
If we have only one node in the DAG, then the total number of ways to form a tree (containing all the vertices) will be 1.
Suppose we want to insert a node v into the DAG, let t be the number of ways to form a tree out of the DAG without v. Then the number of ways to create a tree out of the DAG with v in it will be t*n[v], because for each of the t trees there are n[v] ways to insert the node v in the tree. So if we insert the vertices according to their topological order into the DAG, the total number of trees possible will be 1*n[0]*n[1]*n[2]‚Ä¶n[N-1].  

Road Quality:

If we can find the a tree which maximizes the minimum cost edge, with cost k, all the edges in that tree and all such trees will have their edge cost greater than k. So first we find 'k' in the maximum(in terms of quality) spanning tree and then remove all the edges with edge weight lesser than k. Let the new graph formed be G'. As the maximum spanning tree had all its edge costs greater than or equal to k, G' will always be connected. Furthermore any spanning tree of the graph G' will have the minimum cost edge weight equal to k. Now we can find the minimum(in terms of length) spanning tree of G'. Using disjoint set data structure, we can do everything in O(nlogn) (the time taken to find the MST is dominated by the time taken for sorting) time.

SMS:

We only need to find the number of distinct words that can be formed with a contiguous sequence of repeated values and multiply all of these values as the final answer. The obvious method is that for each number in the sequence, we have two options, inserting a space before it or not inserting a space before it. Then for a sequence of length k, the total number of different words that can be formed out of it is 2^k. But this also considers duplicated, for e.g 6666_66_6 (mnm) is just as same as 6_66_6666. 
We only need to consider the shortest way of forming the current letter so we have the maximum number of possibilities for the previous letter, 
let ƒ[k] be the number of distinct words formed with k digits where there are three letters possible (we can calculate it in a similar way when there are for letters possible). ƒ[0] = 0, ƒ[1] = 1 , ƒ[2]= 2,
ƒ[k] = 1 (consider the entire sequence as one letter) + ƒ[k-1] (e.g: if the last letter is 'm') + ƒ[k-2] (last letter 'n')  + ƒ[k-3] (last letter 'o').
The total time complexity is O(N).

Paths:

We just have to find the shortest path form S to T in O(ElogV) time.

Crazy Walks:

When we want to get at a point 'i', we have two choices of getting there, either jump from any point i-k where the last jump was of length k at the cost of 1, or jump from any point at the cost of Q. Let ƒ[i][k] denote the cost of getting to 'i' with a jump of length k. To get to i with a jump of length k, we will have to jump from i-k, so: ƒ[i][k] = min ( ƒ[i-k][k] + 1,  ƒ[i-k][j] + Q for all j ≤ K ).  This recurrence is correct because to go to i with a jump of k with minimum cost, we first have to get to i-k with minimum cost otherwise we can always find a better way to get to i. What is the running time of this algorithm ? Do we need to check all the j's to find the one which minimizes ƒ[i-k][j] ? We can keep track of the best cost to get at each point, best[i], and update it as we find a better way to get to i. So the new recurrence is  ƒ[i][k] = min ( ƒ[i-k][k] + 1,  best[i-k] + Q) and if the point i is blocked then we cannot get there and we don't want any future points to consider the cost of jumping from i, so we set ƒ[i][k] to infinity. Thus, the total running time is O(N*K).

Dragons:

If d[i][k] is the minimum distance travelled while killing k dragons till the i th row, and l[i][k] is the last dragon we killed, then we can say that d[i][k] is the minimum of the distance travelled if we don't kill the i th dragon (d[i-1][k]) and the distance travelled we kill k-1 dragons optimally and then kill the i th dragon (d[i-1][k-1] + dist(i,last[i-1][k-1])).  

This statement is not true, because if there was some other way of killing k-1  dragons till i-1, t[i-1][k-1], which is not necessarily optimal but the the last dragon we kill was such that the distance between the last dragon kill and i th dragon plus t[i-1][k-1] was much lesser than what we have, d[i][k] cannot be optimal. But if search among all the possible last dragons when killing k-1 dragons and take the one that minimized the distance travelled while  killing k dragons then we have the recurrence:
(ƒ[i][k] is the distance travelled while killing k dragons when the last dragon we kill is i)
ƒ[i][k] =  min ƒ[j][k-1] + dist(i,j) for all j < i
This is correct because if there was some other way to kill k-1 dragons with the last dragon being j, t[j][k-1] which is better than ƒ[j][k-1]
then ƒ[j][k-1] wouldn't have been the optimal, thus we have a contradiction. The running time of the first algorithm was O(N^2) which is faster than that of the current algorithm O(N^3) but the N ≤ 200 so this is indeed optimal.

Posh:

First we compute all the vertices which are 'upmarket'. Instead of taking every vertex and doing a bfs/dfs to find whether there is any path the leads to a shopping centre, we can reverse all the arcs and do a dfs(or bfs) from the vertices which have shopping centers and mark all the vertices we can reach as upmarket. If we read a vertex which is already marked upmarket, we don't need to traverse any further because we have already explored the vertices reachable from it. Once we have all the upmarket vertices, we do a dfs from the vertices which are not upmarket (keeping the arcs reversed) and mark all the vertices reachable from them as 'not posh' all the vertices not marked are 'posh'. Both the dfs take linear tine as we mark a vertex at most once and so the total complexity is O(N).

Radio:

We maintain a sliding window and a binary heap to help us find the maximum. At any point of the algorithm we have exactly K elements in the window. There will be R elements in the heap. Every element 'r' in the heap stores the number of occurrences of  r in the current window. Initially every r is set to the number of occurrences of r from the position 0 to K-1 in the array. At each step we decrease the number of occurrences of the leftmost element and contract the window from the left (deleting the leftmost element), then we expand it one element to the right and increase the occurrences of the newly added element and report the root (the maximum element) of the heap. We can implement the above procedures in O(logR) time and so the total running time is O(NlogR).

Microscope:

As the order of viewing the elements is specified, we simply have to move the microscope as little as we can so that the current element is in the view.

Unequal:

We convert each contiguous sequence of same values separately.

Case 1:  Sequence has odd length.
We only change the alternate values in the sequence starting from the second to 0 (1 if the sequence has all 0s). This will not affect any of the neigh boring sequences as we will not change endpoints.

Case 2: Sequence has even length and the element in the sequence are nonzero. 
To get the lexicographically earliest sequence we change alternate elements to 0 starting from the first element.
If the element before the sequence is 0, then we start with the second element. If the element after the sequence is also 0, the the last element gets merged into the next sequence.

Case 3: Sequence has even length and the elements in it are zero.
In this case, we change the elements to one alternatively staring from the second element to get the lexicographically smallest sequence.
If the element after the current sequence is 1, then the last element of the sequence gets merged with the new sequence.

We can count the changes as we make them (we never change an element twice even if it gets merged with the next sequence) or after we have constructed the new sequence, we can check with the old sequence and count the number of values changed. Two sequences will have at most one element in common so the total running time of the algorithm is O(N).

Guru:

We don't need to calculate the actual average only the index at which it is the maximum. For all the indices from 0 to N-1, we compute C[i] which is the ratio of the sum of all numerators from 0 to i and the sum of all denominators staring from 0 to i.  We create an array M[] which stores the temporary maxima C[i]s as we go from N-1 to 0. Now we have all the indices which maximize the average in reverse order. M[k] will be the current index which maximizes C[i]. Initially k is set to the last index of m. Whenever i > M[k], we decrement k. Total runtime O(N).

